# -*- coding: utf-8 -*-
"""Lafite-demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MdnrMwpQajudJG4R3F54ii9tl1rcrLbJ
"""

text_input = 'a lake on top of the mountain' #@param {type: "string"}

num_images_to_generate = 3 #@param {type: "number"}

text_input = 'A boy brushing his teeth near a window' #@param {type: "string"}

num_images_to_generate = 1 #@param {type: "number"}



from google.colab import drive
drive.mount('/content/drive/')

from pandas.io import excel
import csv
import pandas as pd
import xlrd
path="/content/race-bias.txt"
# path='/content/drive/MyDrive/Project-AML/face_captions.txt'
# path='/content/drive/MyDrive/Project-AML/bias_captions.txt'
with open(path) as f:
    lines = f.readlines()
print(lines)
num_images_to_generate = 1 #@param {type: "number"}

lines2=[]
for line in lines:
  lines2.append(line.split("\n")[0])
print(lines2)

# Commented out IPython magic to ensure Python compatibility.
!pip install torch==1.9.1 torchtext==0.10.1 torchvision==0.10.1 torchaudio==0.10.1
# %cd /content
!git clone https://github.com/drboog/Lafite
# %cd /content/Lafite
!pip install git+https://github.com/openai/CLIP.git
!pip install ninja

!wget -N https://public-ipfs-gateway.pollinations.ai/ipfs/Qmdt4rch9AHveSh9JohEHqUGFMdfThsXJknwHjtQUQJQhe/pre-trained-google-cc-best-fid.pkl

import torch
import numpy as np
import pickle
import os
import clip
from PIL import Image
from pathlib import Path
from tqdm import tqdm
import dnnlib, legacy
import clip
import torch.nn.functional as F
import torchvision.transforms as T
from tqdm import tqdm
import scipy

class Generator:
    def __init__(self, device, path):
        self.name = 'generator'
        self.model = self.load_model(device, path)
        self.device = device
        self.force_32 = False

    def load_model(self, device, path):
        with dnnlib.util.open_url(path) as f:
            network= legacy.load_network_pkl(f)
            self.G_ema = network['G_ema'].to(device)
            self.D = network['D'].to(device)
#                 self.G = network['G'].to(device)
            return self.G_ema

    def generate(self, z, c, fts, noise_mode='const', return_styles=True):
        return self.model(z, c, fts=fts, noise_mode=noise_mode, return_styles=return_styles, force_fp32=self.force_32)

    def generate_from_style(self, style, noise_mode='const'):
        ws = torch.randn(1, self.model.num_ws, 512)
        return self.model.synthesis(ws, fts=None, styles=style, noise_mode=noise_mode, force_fp32=self.force_32)

    def tensor_to_img(self, tensor):
        img = torch.clamp((tensor + 1.) * 127.5, 0., 255.)
        img_list = img.permute(0, 2, 3, 1)
        img_list = [img for img in img_list]
        return Image.fromarray(torch.cat(img_list, dim=-2).detach().cpu().numpy().astype(np.uint8))

for f in range(1,16):

  with torch.no_grad():

      device = 'cuda:0' # please use GPU, do not use CPU
    #  path = '/content/Lafite/pre-trained-google-cc-best-fid.pkl'  # pre-trained model
      # path = '/content/drive/MyDrive/Project-AML/COCO2014_Language-free_Gaussian.pkl'
      path = '/content/drive/MyDrive/Project-AML/COCO2014_Language-free-NN.pkl' # second model
      # path = '/content/drive/MyDrive/Project-AML/COCO2014_CLIP_ViTB32_all_text.pkl'#third model
      # path='/content/drive/MyDrive/Project-AML/COCO2014_CLIP_ViTB16_best_FID_8.12.pkl'#fourth model
      #path='/content/drive/MyDrive/Project-AML/pre-trained-google-cc-best-fid.pkl'#fourth model

      generator = Generator(device=device, path=path)
      clip_model, _ = clip.load("ViT-B/32", device=device)
      clip_model = clip_model.eval()

      for img_num in range(0,len(lines2)-1):
        for text_input in lines2:
          tokenized_text = clip.tokenize(lines2[img_num]*num_images_to_generate).to(device)
          tokenized_text = clip.tokenize(text_input[:-1]*num_images_to_generate).to(device)
          txt_fts = clip_model.encode_text(tokenized_text)
          txt_fts = txt_fts/txt_fts.norm(dim=-1, keepdim=True)

          z = torch.randn((num_images_to_generate, 512)).to(device)
          c = torch.randn((num_images_to_generate, 1)).to(device) # label is actually not used
          img, _ = generator.generate(z=z, c=c, fts=txt_fts)
          to_show_img = generator.tensor_to_img(img)
          # to_show_img.save('/content/drive/MyDrive/Project-AML/Demo-Images/%s.jpg' % text_input[:-1])
          # to_show_img.save('/content/drive/MyDrive/Project-AML/10kimage-Lafite-NN/image_'+str(img_num)+'.jpg')
          # to_show_img.save('/content/drive/MyDrive/Project-AML/Bias/%s.jpg' % text_input[:-1])
          to_show_img.save('/content/drive/MyDrive/Project-AML/New Bias/Lafite/Race'+text_input+str(f)+'.jpg')

"""moving images to the right folder"""

path='/content/drive/MyDrive/Project-AML/New Bias/Stable diffusion'
dest_dir='/content/drive/MyDrive/Project-AML/New Bias/Stable diffusion/Race'
import shutil
import os

for img in os.listdir(path):
  if img.endswith(".jpg") and not(os.path.isdir(os.path.join(path,img))):
    # os.remove(os.path.join(path,img))
    # new_name = img.split("LAFITE_Flickr30_motion")[1]
    shutil.move(os.path.join(path,img), os.path.join(dest_dir, img))

len(os.listdir(path))

" Young women taking a break from shopping .jpg" in os.listdir(dest_dir)

