# -*- coding: utf-8 -*-
"""R-precision/AttGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mKLOLaUw-wDscEZJAdzNqT4Uk3Ze8zAZ
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.insert(0, '/content/drive/MyDrive/Project-AML/R-Precision/AttnGAN-master/code')

import config as cfg

"""Image *Encoder*"""

from torchvision import models, transforms
from PIL import Image

def load_image(input_image_path,size):
  input_image = Image.open(input_image_path).convert('RGB')
  # Transform the image, so it becomes readable with the model
  transform = transforms.Compose([
    transforms.CenterCrop(224),
    transforms.Resize(size),
    transforms.ToTensor()
  ])
  # Apply the transforms to the input image
  return transform(input_image)

#image preprocessing and then encoding
import torch
from model import CNN_ENCODER

# Load the pre-trained image encoder
state_dict = torch.load('/content/drive/MyDrive/Project-AML/R-Precision/coco/image_encoder100.pth', map_location=lambda storage, loc: storage)
image_encoder = CNN_ENCODER(256)
image_encoder.load_state_dict(state_dict)

# Preprocess the input image
def preprocess_img(img_path):#function to get the image embeddings given the image path
# img_path = "/content/drive/MyDrive/Project-AML/DemoImagesCOCO/A boy eating food off an orange plate.jpg"
  img= load_image(img_path,256)
# print(img.type())
# Encode the input image using the pre-trained image encoder
  with torch.no_grad():
      img_tensor = img.unsqueeze(0)
      image_embedding = image_encoder(img_tensor)
  return image_embedding[1]

"""Text Encoder"""

import nltk

nltk.download("all")

from collections import Counter
from nltk.tokenize import word_tokenize

# Sample sentences

# def wordtoix_dictionary_builder(sentences):
#   # Tokenize the sentences
#   tokens = [word_tokenize(sent.lower()) for sent in sentences]

#   # Flatten the list of tokens
#   all_tokens = [token for sent_tokens in tokens for token in sent_tokens]

#   # Count the occurrences of each token
#   token_counter = Counter(all_tokens)

#   # Create the wordtoix dictionary
#   wordtoix = {word: idx for idx, (word, _) in enumerate(token_counter.items())}

#   return wordtoix

"""Revised version of the function above to include the unknown token:"""

def wordtoix_dictionary_builder(sentences):
    # Tokenize the sentences
    tokens = [word_tokenize(sent.lower()) for sent in sentences]

    # Flatten the list of tokens
    all_tokens = [token for sent_tokens in tokens for token in sent_tokens]

    # Count the occurrences of each token
    token_counter = Counter(all_tokens)

    # Create the wordtoix dictionary
    wordtoix = {word: idx+1 for idx, (word, _) in enumerate(token_counter.items())}

    # Add <UNK> token to the dictionary with index 0
    wordtoix['<UNK>'] = 0

    return wordtoix

from model import RNN_ENCODER
text_encoder =RNN_ENCODER(27297, ninput=300, drop_prob=0.5,
                 nhidden=256, nlayers=1, bidirectional=True)
state_dict =torch.load('/content/drive/MyDrive/Project-AML/R-Precision/coco/text_encoder100.pth',
                map_location=lambda storage, loc: storage)
text_encoder.load_state_dict(state_dict)
for p in text_encoder.parameters():
    p.requires_grad = False
text_encoder.eval()

# def generate_text_embedding(caption, text_encoder, wordtoix):
#     from torch.autograd import Variable

#     # Tokenize the caption
#     tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
#     tokens = tokenizer.tokenize(caption.lower())

#     # Convert the tokens to indices using the wordtoix dictionary
#     caption_indices = [wordtoix[word] for word in tokens if word in wordtoix]

#     # Convert the indices to a tensor and add a batch dimension
#     caption_tensor = torch.tensor(caption_indices, dtype=torch.int64).unsqueeze(0)

#     # Get the length of the caption and add a batch dimension
#     caption_length = torch.tensor([len(caption_indices)], dtype=torch.int64)

#     # Initialize the hidden state for the RNN_ENCODER
#     batch_size = 1
#     hidden = text_encoder.init_hidden(batch_size)

#     # Pass the caption tensor and length through the RNN_ENCODER to generate text embeddings
#     _, sent_emb = text_encoder(Variable(caption_tensor), Variable(caption_length), hidden)

#     # Detach the sentence-level embedding
#     sent_emb = sent_emb.detach()

#     return sent_emb

"""Revised version of above function to add the unkown tokens embedding:"""

def generate_text_embedding(caption, text_encoder, wordtoix, min_length=3):
    from torch.autograd import Variable

    # Tokenize the caption
    tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(caption.lower())

    # print("Tokens:", tokens)  # Add this line to display the tokens

    # Check if the token length is less than the minimum length
    if len(tokens) < min_length:
        tokens += ['<PAD>'] * (min_length - len(tokens))

    # Convert the tokens to indices using the wordtoix dictionary
    caption_indices = [wordtoix[word] if word in wordtoix else wordtoix['<UNK>'] for word in tokens]

    unknown_words = [word for word in tokens if word not in wordtoix]
    if unknown_words:
        print("Unknown words:", unknown_words)  # Display the unknown words if any

    # Convert the indices to a tensor and add a batch dimension
    caption_tensor = torch.tensor(caption_indices, dtype=torch.int64).unsqueeze(0)

    # Get the length of the caption and add a batch dimension
    caption_length = torch.tensor([len(caption_indices)], dtype=torch.int64)

    # Initialize the hidden state for the RNN_ENCODER
    batch_size = 1
    hidden = text_encoder.init_hidden(batch_size)

    # Pass the caption tensor and length through the RNN_ENCODER to generate text embeddings
    _, sent_emb = text_encoder(Variable(caption_tensor), Variable(caption_length), hidden)

    # Detach the sentence-level embedding
    sent_emb = sent_emb.detach()

    return sent_emb

# def generate_text_embedding(caption, text_encoder, wordtoix):
#     from torch.autograd import Variable

#     # Tokenize the caption
#     tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
#     tokens = tokenizer.tokenize(caption.lower())

#     # Convert the tokens to indices using the wordtoix dictionary
#     caption_indices = [wordtoix[word] if word in wordtoix else wordtoix['<UNK>'] for word in tokens]

#     # Convert the indices to a tensor and add a batch dimension
#     caption_tensor = torch.tensor(caption_indices, dtype=torch.int64).unsqueeze(0)

#     # Get the length of the caption and add a batch dimension
#     caption_length = torch.tensor([len(caption_indices)], dtype=torch.int64)

#     # Initialize the hidden state for the RNN_ENCODER
#     batch_size = 1
#     hidden = text_encoder.init_hidden(batch_size)

#     # Pass the caption tensor and length through the RNN_ENCODER to generate text embeddings
#     _, sent_emb = text_encoder(Variable(caption_tensor), Variable(caption_length), hidden)

#     # Detach the sentence-level embedding
#     sent_emb = sent_emb.detach()

#     return sent_emb

"""R-Precision"""

#COCO captions faces
with open("/content/drive/MyDrive/Project-AML/face_captions_COCO.txt") as file:
    listt = [line.strip().replace('.', '') for line in file.readlines()]

#Flickr captions -faces
with open("/content/drive/MyDrive/Project-AML/flickr30k_images/Flickr30_captions_faces.txt") as file:
    listt = [line.strip().replace('.', '') for line in file.readlines()]

#Flickr captions -motion
with open("/content/drive/MyDrive/Project-AML/flickr30k_images/Flickr30_captions_motions.txt") as file:
    listt = [line.strip().replace('.', '') for line in file.readlines()]
/content/drive/MyDrive/Project-AML/COCO_motion_captions.txt

#COCO captions motion
with open("/content/drive/MyDrive/Project-AML/COCO_motion_captions.txt") as file:
    listt = [line.strip().replace('.', '') for line in file.readlines()]

len(listt)

wordtoix=wordtoix_dictionary_builder(listt)

wordtoix['<PAD>'] = 0

"""Generating the random captions from the dataset

"""

import random
random_captions=[]
text_embeddings=[]
for i in range(0, 99):
      y = random.randrange(len(listt))
      text_embeddings.append(generate_text_embedding(listt[y], text_encoder, wordtoix))

import numpy as np
import torch.nn as nn
from sklearn.metrics.pairwise import cosine_similarity


# image_embeddings = np.vstack(image_embedding[1])
# text_embeddings = np.vstack(text_embedding)
# Calculate R-precision score

# similarities=[]
# ground_truth_sim=cosine_similarity(ground_truth_caption_embed, image_embedding[1])
# similarities.append(ground_truth_sim[0][0])
def R_precision_score(ground_truth_caption,text_embeddings,Image_embedding):
  similarities=[]
  ground_truth_caption_embed=generate_text_embedding(ground_truth_caption, text_encoder, wordtoix)# generate the ground truth caption embedding using the text encoder
  ground_truth_sim=cosine_similarity(ground_truth_caption_embed, Image_embedding)
  similarities.append(ground_truth_sim[0][0])
  r_precision_sum=0
  for caption in text_embeddings:
    cos_sim=cosine_similarity(caption, Image_embedding)[0][0]
    similarities.append(cos_sim)
  # Find the index of the ground truth caption in the sorted list
  # Convert the list to a NumPy array and negate it
  sorted_indices = np.argsort(-np.array(similarities))
  rank = np.where(sorted_indices == 2)
  r_precision = 1 / (rank[0][0] + 1)
  r_precision_sum += r_precision
  return r_precision_sum

# R_precision_score(text_embeddings, image_embedding[1])

"""General function to load all the images and get the groud truth caption from their name and also calls the R_precision_score over the 99 random captions and the images with their ground truth caption."""

#image path stable diffusion coco
img_path="/content/drive/MyDrive/Project-AML/Stable diffusion/Gen_Faces_Diffusion"

#image path Dall e mini coco
img_path="/content/drive/MyDrive/Project-AML/DALL E mini/10k generated faces"

#image path Lafite coco faces
img_path="/content/drive/MyDrive/Project-AML/10kimages"

#image path lafite coco motions
img_path="/content/drive/MyDrive/Lafite-NN"

#image path Stable diffusion Flickr30k faces
img_path="/content/drive/MyDrive/Project-AML/Stable diffusion/Flickr30k/New_17k_Face_Motion_Unfiltered/Gen_Faces"

#image path LAFITE Flickr30k motion
img_path="/content/drive/MyDrive/Project-AML/LAFITE_Flickr30_motion"

len(os.listdir(img_path))

import os

def main_func(img_folder_path):#run this function only by giving the path to the synthesized image of any model

  sum_r_prec, avg_r_prec, count=0,0,0

  for img in os.listdir(img_folder_path):#loop over the images inside the img_folder_path
    if count<len(listt)-1:
        img_path=os.path.join(img_folder_path,img)#get the image path for the next function
        Image_embedding=preprocess_img(img_path)#get the image embeddings using the image encoder given the img_path
        # ground_truth_caption=img.split(".jpg")[0] #get the ground truth caption from the image file name
        ground_truth_caption=listt[count] #need to be check with a sample #listt is the list of face_captions
        sum_r_prec += R_precision_score(ground_truth_caption,text_embeddings,Image_embedding)  #run the r-precision function with the ground truth caption and the 99 random text_embeddings
        count += 1 #counting the number of images in the Image folder

  avg_r_prec=sum_r_prec/count

  return avg_r_prec

#LAFITE Flickr30 R-precision
main_func(img_path)

"""#LAFITE Flickr30 R-precision

0.05077807325816846
"""

#Stable diffusion COCO R-precision
main_func(img_path)

"""#Stable diffusion COCO R-precision
0.022572995433199114
"""

#Dall-E mini COCO R-precision
main_func(img_path)

"""#Dall-E mini COCO R-precision
0.021580540754460847
"""

#LAFITE COCO R-precision
main_func(img_path)

"""#LAFITE COCO R-precision
0.01570107598581733
"""

#Stable diffusion Flickr30 R-precision
main_func(img_path)

#LAFITE Flickr30k motion R-precision
main_func(img_path)

"""#LAFITE Flickr30k motion R-precision
0.014820932885802485
"""

#LAFITE COCO motion R-precision
main_func(img_path)

"""#LAFITE COCO motion R-precision
0.01440743343793611

# stable diffusion coco motion R-precision
0.01379336073406082
"""

